{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather LinkedIn URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests # allows us to fetch URLs\n",
    "import unicodedata # allows unicode transformations\n",
    "import re # regex for advanced string transformations\n",
    "\n",
    "# This function simply creates the url, in string form, and stores it to the list of urls\n",
    "def make_lnkd_urls(username, url_list):\n",
    "    url_list.append(\"https://www.linkedin.com/in/\" + username)\n",
    "\n",
    "# Add any other parameters here in this function\n",
    "## parameters should/could be columns in your csv file\n",
    "def get_lnkd_username(name, title, company, url_list, no_list):\n",
    "    name = name.replace(\" \", \"+\").replace(\"_\", \"\").encode('utf-8').strip()\n",
    "    title = title.replace(\" \", \"+\").encode('utf-8').strip()\n",
    "    company = company.replace(\" \", \"+\").encode('utf-8').strip()\n",
    "\n",
    "    # EDIT this search string based on the parameters of the function\n",
    "    url = \"https://www.bing.com/search?q=\" + name + \"+\" + title + \"+\" + company + \"linkedin\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    #print response to check for errors\n",
    "    print response\n",
    "    \n",
    "    string_v = unicodedata.normalize('NFKD', response.text).encode('ascii','ignore')\n",
    "    m = re.search('https://www.linkedin.com/in/(.+?)\"', string_v)\n",
    "    \n",
    "    if m:\n",
    "        try:\n",
    "            make_lnkd_urls(m.group(1), url_list)  \n",
    "        except AttributeError:\n",
    "            no_list.append(name)\n",
    "            print \"Nothing for: \", name\n",
    "    else:\n",
    "        # if name does not exist, this widens the search query by dropping a parameter\n",
    "        ## In this case, I dropped company, but feel free to drop whichever you see fit\n",
    "        url_2 = \"https://www.bing.com/search?q=\" + name + \"+\" + title + \"linkedin\"\n",
    "        response = requests.get(url)\n",
    "        print response\n",
    "        string_v = unicodedata.normalize('NFKD', response.text).encode('ascii','ignore')\n",
    "        n = re.search('https://www.linkedin.com/in/(.+?)\"', string_v)\n",
    "        if n:\n",
    "            try:\n",
    "                make_lnkd_urls(m.group(1), url_list)  \n",
    "            except AttributeError:\n",
    "                no_list.append(name)\n",
    "                print \"Nothing for: \", name\n",
    "        else:\n",
    "            no_list.append(name)\n",
    "            print \"Nothing for: \", name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run search query to gather list of LinkedIn URLs for each of the names in the CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas\n",
    "\n",
    "raw_row_list = []\n",
    "# **Replace file path with your own!**\n",
    "csv_file = csv.reader(open(\"/something/something/data.csv\", \"rb\"), delimiter=\",\")\n",
    "for row in csv_file:\n",
    "    raw_row_list.append(row)\n",
    "\n",
    "master_row_list = raw_row_list[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time # for making variable time delays between searches\n",
    "import random\n",
    "\n",
    "url_list = []\n",
    "no_list= []\n",
    "\n",
    "for row in master_row_list:\n",
    "    get_lnkd_username(row[0], row[1], url_list, no_list)\n",
    "    i = random.uniform(0.7, 3.4)\n",
    "    time.sleep(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## May have to rerun functions a few times since Bing times out (just start from the last name before time out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DONT RUN THIS BLOCK IF YOU DIDNT TIME OUT\n",
    "## It's here for your convenience\n",
    "\n",
    "import time # for making variable time delays between searches\n",
    "import random\n",
    "\n",
    "url2_list = []\n",
    "no2_list= []\n",
    "\n",
    "for row in master_row_list[542:]:\n",
    "    get_lnkd_username(row[0], row[1], url2_list, no2_list)\n",
    "    b = random.uniform(0.7, 3.4)\n",
    "    time.sleep(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DONT RUN THIS BLOCK IF YOU DIDNT TIME OUT\n",
    "## It's here for your convenience\n",
    "\n",
    "import time # for making variable time delays between searches\n",
    "import random\n",
    "\n",
    "url3_list = []\n",
    "no3_list= []\n",
    "\n",
    "for row in master_row_list[1043:]:\n",
    "    get_lnkd_username(row[0], row[1], url3_list, no3_list)\n",
    "    b = random.uniform(0.7, 3.4)\n",
    "    time.sleep(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After running through all the names, combine lists\n",
    "### Even if Bing didn't time out, run anyways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# combine all runs into one list\n",
    "\n",
    "master_url_list = url_list + url2_list + url3_list\n",
    "master_no_list = no_list + no2_list + no3_list\n",
    "print len(master_url_list)\n",
    "print len(master_no_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to master_url_list to CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# master_url_list for safekeeping\n",
    "\n",
    "import csv\n",
    "# **Replace file path with your own!**\n",
    "with open(\"/Users/david.wang/Google_Drive/Hackathon/master_url_list.csv\", \"wb\") as csvfile:\n",
    "    ticket_writer = csv.writer(csvfile)\n",
    "    ticket_writer.writerow(['urls'])\n",
    "    for row in master_url_list:\n",
    "        ticket_writer.writerow([row,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BeautifulSoup and Selenium for scraping profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# divis by 2 function to switch web drivers on each iteration through master_url_list\n",
    "\n",
    "def by_two (num):\n",
    "    if num % 2 == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "\n",
    "past_companies2 = []\n",
    "bad_urls2 = []\n",
    "\n",
    "count = 0\n",
    "\n",
    "for url in master_url_list:\n",
    "    \n",
    "    # set the counter for web driver switcher\n",
    "    count += 1\n",
    "    \n",
    "    if count%50 == 0:\n",
    "        print 'Status Update: PC: ', len(past_companies2)\n",
    "        print 'Status Update: BU: ', len(bad_urls2)\n",
    "    \n",
    "    # set randomized time delay between iterations\n",
    "    i = random.uniform(1.3, 4.2)\n",
    "    time.sleep(i)\n",
    "\n",
    "    # web driver switcher\n",
    "    if by_two(count):\n",
    "        driver = webdriver.Firefox()\n",
    "    else:\n",
    "        driver = webdriver.Chrome()\n",
    "    \n",
    "    # get the profile using url\n",
    "    try:\n",
    "        driver.get(url) # use driver to get profile response\n",
    "    except:\n",
    "        bad_urls2.append(url)\n",
    "        continue\n",
    "        \n",
    "    html = driver.page_source # load driver to html\n",
    "    soup = BeautifulSoup(html) # use BS on html\n",
    "    \n",
    "    # search for the \"Previous positions\" section on the header section of the linkedin profile\n",
    "    ## within the tr tag, where data-section field is pastPositionDetails\n",
    "    previous = soup.find('tr', { \"data-section\" : \"pastPositionsDetails\" })\n",
    "    \n",
    "    ### For more positions, inspect any public profile for the correct tags\n",
    "    \n",
    "    # try to see if past positions are listed\n",
    "    try:\n",
    "        text_v = previous.getText().encode('utf-8').strip().replace(\"Previous\", \"\").split(\",\")\n",
    "    except:\n",
    "        bad_urls2.append(url)\n",
    "        driver.close()\n",
    "        continue\n",
    "    \n",
    "    # append text_v to past_companies2\n",
    "    if text_v[0] == company or text_v[0] == company:\n",
    "        try:\n",
    "            past_companies2.append(text_v[1])\n",
    "        except:\n",
    "            bad_urls2.append(url)\n",
    "    else:\n",
    "        past_companies2.append(text_v[0])\n",
    "    driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(past_companies2)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Companies to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# past_companies\n",
    "\n",
    "import csv\n",
    "# **Replace file path with your own!**\n",
    "with open(\"/Volumes/home/w/wang.david/past_companies.csv\", \"wb\") as csvfile:\n",
    "    ticket_writer = csv.writer(csvfile)\n",
    "    ticket_writer.writerow(['companies'])\n",
    "    for row in past_companies2:\n",
    "        ticket_writer.writerow([row,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
